{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4 import BeautifulStoneSoup\n",
    "import nltk.data\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import pprint\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stopwords = nltk.corpus.stopwords.words('english')+[',','.','!','``',\"''\",'?',\"'s\",';','$',':',\"'\",'_', ')', '(']\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_futureWork(all_text):\n",
    "    ret = \"\"\n",
    "    pattern = re.compile(\"future\")\n",
    "    for sent in sent_detector.tokenize(all_text.strip()):\n",
    "        clean_sent = sent.lower().replace('\\n', ' ').strip()\n",
    "        clean_sent = clean_sent.replace(\"- \", \"\")\n",
    "        if pattern.search(clean_sent):\n",
    "            ret = ret + \"\\n\" + clean_sent\n",
    "\n",
    "    return ret\n",
    "\n",
    "def get_futureWork_plus(all_text):\n",
    "    ret = \"\"\n",
    "    pattern1 = re.compile(\"future\")\n",
    "    pattern2 = re.compile(\"plan to\")\n",
    "    for sent in sent_detector.tokenize(all_text.strip()):\n",
    "        clean_sent = sent.lower().replace('\\n', ' ').strip()\n",
    "        clean_sent = clean_sent.replace(\"- \", \"\")\n",
    "        if pattern1.search(clean_sent):\n",
    "            ret = ret + \"\\n\" + clean_sent\n",
    "        \n",
    "        #todo make sure the sentence is not added already\n",
    "        elif pattern2.search(clean_sent):\n",
    "            ret = ret + \"\\n\" + clean_sent\n",
    "            \n",
    "    return ret\n",
    "\n",
    "def get_futureWork_extended(all_text, indicators):\n",
    "    ret = \"\"\n",
    "    #indicators = ['futur', 'work', 'use', 'plan', 'model', 'improv', 'system', 'research', 'method', 'featur', 'includ', 'investig', 'explor', 'direct', 'languag', 'would', 'data', 'evalu', 'approach', 'perform']\n",
    "    #indicators = ['futur', 'work', 'plan', 'improv', 'explor', 'approach', 'perform', 'research', 'evalu', 'extend', 'would']\n",
    "    for sent in sent_detector.tokenize(all_text.strip()):\n",
    "        clean_sent = sent.lower().replace('\\n', ' ').strip()\n",
    "        clean_sent = clean_sent.replace(\"- \", \"\")\n",
    "        \n",
    "        for token in [t.lower() for t in nltk.word_tokenize(clean_sent)]:\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "            if stemmer:\n",
    "                token = stemmer.stem(token)\n",
    "            \n",
    "            if token in indicators:\n",
    "                ret = ret + \"\\n\" + clean_sent\n",
    "                break\n",
    "            \n",
    "    return ret\n",
    "\n",
    "def get_abstract(all_text):\n",
    "    ret = \"\"\n",
    "    for sent in sent_detector.tokenize(all_text.strip()):\n",
    "        clean_sent = sent.lower().replace('\\n', ' ').strip()\n",
    "        clean_sent = clean_sent.replace(\"- \", \"\")\n",
    "        ret = ret + \"\\n\" + clean_sent\n",
    "\n",
    "    return ret\n",
    "\n",
    "#Gets all the body texts until the next section header.\n",
    "def get_bodyText(starterPointer):\n",
    "    saveText = \" \"\n",
    "    if starterPointer is not None:\n",
    "        #saveText = starterPointer.get_text()\n",
    "        currentPoint = starterPointer.find_next()\n",
    "        while currentPoint is not None :\n",
    "            #print currentPoint.name\n",
    "            if currentPoint.name == 'sectionHeader':\n",
    "                break\n",
    "            #elif currentPoint.name == 'page':\n",
    "            #    print currentPoint.name\n",
    "            elif currentPoint.name == 'bodyText':\n",
    "                saveText = saveText + currentPoint.get_text()\n",
    "\n",
    "            currentPoint = currentPoint.find_next()\n",
    "            \n",
    "    #print saveText\n",
    "    return saveText\n",
    "\n",
    "def prev_bodyText(starterPointer):\n",
    "    saveText = \" \"\n",
    "    if starterPointer is not None:\n",
    "        saveText = starterPointer.get_text()\n",
    "        currentPoint = starterPointer.find_previous()\n",
    "        while currentPoint is not None :\n",
    "            #print currentPoint.name\n",
    "            if currentPoint.name == 'sectionHeader':\n",
    "                break\n",
    "            #elif currentPoint.name == 'page':\n",
    "            #    print currentPoint.name\n",
    "            elif currentPoint.name == 'bodyText':\n",
    "                saveText = saveText + currentPoint.get_text()\n",
    "\n",
    "            currentPoint = currentPoint.find_previous()\n",
    "    return saveText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how many for which we have labels (summary, machine translation, dependency parsing) are in the cleanXMLdata?\n",
    "cleanData = glob.glob(\"../cleanXMLdataV2/*.out\")\n",
    "cd_files = [ i.split('/')[-1].split('.')[0] for i in cleanData]\n",
    "labelData = glob.glob(\"../aan/aan_mds/papers_text/*\")\n",
    "ld_files = [ i.split('/')[-1] for i in labelData]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 20399)\n"
     ]
    }
   ],
   "source": [
    "print (len(ld_files), len(cd_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these are the files for which we have labels\n",
    "common_files = set(ld_files).intersection(set(cd_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def print_sentences(some_text):\n",
    "    sentences = []\n",
    "    sent_list = tokenizer.tokenize(some_text)\n",
    "    #sent_list = some_text.split('.')\n",
    "    for i, sent in enumerate( sent_list ):\n",
    "        if len(sent.strip()) > 5:\n",
    "            #print (i,sent.strip())\n",
    "            sentences.append(sent.replace('\\n', ' ').strip())\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_aan_metadata():\n",
    "    paperID2label = {}\n",
    "    paperID2title = {}\n",
    "    \n",
    "    with open ( '../aan/aan_mds/metadata.txt', 'r') as f:\n",
    "        meta_data = f.readlines()\n",
    "    for line in meta_data:\n",
    "        #print (line)\n",
    "        item = line.split('\\t')\n",
    "        paperID2label [ item[1] ] = item[0]\n",
    "        paperID2title [ item[1] ] = item[2]\n",
    "    return paperID2label , paperID2title\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E89-1038\n",
      "C86-1117\n",
      "H01-1007\n",
      "W05-0712\n",
      "C80-1064\n",
      "W07-0724\n",
      "C86-1153\n",
      "P84-1072\n",
      "W06-3121\n",
      "W06-3123\n",
      "W05-0812\n",
      "P03-1005\n",
      "P00-1006\n",
      "W07-0717\n",
      "P98-1070\n",
      "C00-2162\n",
      "H91-1026\n",
      "P07-1089\n",
      "C92-2101\n",
      "C94-2178\n",
      "C92-4203\n",
      "C86-1100\n",
      "N06-1058\n",
      "W03-0318\n",
      "P99-1067\n",
      "W01-1406\n",
      "P06-2014\n",
      "C80-1067\n",
      "P02-1044\n",
      "P03-1010\n",
      "C94-1048\n",
      "J93-1006\n",
      "W06-1628\n",
      "H05-1085\n",
      "W07-0734\n",
      "W01-0808\n",
      "D07-1104\n",
      "H92-1052\n",
      "P96-1023\n",
      "C88-1017\n",
      "P06-1065\n",
      "P06-1011\n",
      "C00-2092\n",
      "P98-2139\n",
      "P98-2160\n",
      "J00-2004\n",
      "D07-1103\n",
      "P07-2017\n",
      "P98-1017\n",
      "P08-1114\n",
      "P03-1040\n",
      "C86-1025\n",
      "P93-1004\n",
      "W05-1506\n",
      "C88-2154\n",
      "W03-0413\n",
      "J99-1003\n",
      "P97-1046\n",
      "W06-1609\n",
      "H01-1062\n",
      "P07-1111\n",
      "E93-1062\n",
      "P03-1041\n",
      "P84-1105\n",
      "N06-1014\n",
      "P98-1036\n",
      "C96-2119\n",
      "C02-1065\n",
      "W03-0311\n",
      "P06-1009\n",
      "W97-0311\n",
      "P04-1078\n",
      "W05-0825\n",
      "P01-1067\n",
      "D07-1091\n",
      "N03-1003\n",
      "P91-1022\n",
      "P99-1011\n",
      "W03-0310\n",
      "P03-1012\n",
      "P99-1028\n",
      "W08-0405\n",
      "P98-1069\n",
      "P98-1004\n",
      "C90-3044\n",
      "W00-0507\n",
      "W01-1402\n",
      "C82-1034\n",
      "W07-0414\n",
      "C04-1154\n",
      "J97-3002\n",
      "C00-1019\n",
      "P07-2026\n",
      "H05-1011\n",
      "P91-1021\n",
      "P88-1019\n",
      "C92-2081\n",
      "C92-3168\n",
      "W07-0730\n",
      "E03-1007\n",
      "H93-1038\n",
      "P94-1012\n",
      "P06-1002\n",
      "P95-1032\n",
      "C92-3164\n",
      "P98-1117\n",
      "C90-3057\n",
      "C96-1054\n",
      "C88-1016\n",
      "D07-1092\n",
      "W05-0817\n",
      "P06-2098\n",
      "C96-1067\n",
      "P06-2112\n",
      "E95-1026\n",
      "W03-1608\n",
      "W06-3105\n",
      "P98-2162\n",
      "C92-2092\n",
      "W06-3113\n",
      "H05-1095\n",
      "W06-3601\n",
      "P98-2173\n",
      "P93-1001\n",
      "W07-0728\n",
      "P95-1033\n",
      "W01-0504\n",
      "W06-3112\n",
      "P97-1037\n",
      "C00-2172\n",
      "D07-1089\n",
      "W07-0409\n",
      "C02-1134\n",
      "P07-2054\n",
      "J03-3002\n",
      "P07-1090\n",
      "W05-0908\n",
      "C96-1070\n",
      "J03-1002\n",
      "C82-1004\n",
      "P08-1023\n",
      "P06-2003\n",
      "W03-0301\n",
      "P99-1049\n",
      "P04-1079\n",
      "C96-2141\n",
      "J96-1001\n",
      "W06-3115\n",
      "W02-1610\n",
      "W04-0802\n",
      "W02-1012\n",
      "W07-1424\n",
      "C88-1053\n",
      "C02-1011\n",
      "D07-1006\n",
      "N01-1020\n",
      "N07-1063\n",
      "W05-0833\n",
      "H01-1002\n",
      "N03-2017\n",
      "C92-2117\n",
      "C00-2131\n",
      "J85-2004\n",
      "W05-0909\n",
      "W93-0301\n",
      "C82-1043\n",
      "W02-2026\n",
      "W05-0814\n",
      "J99-4005\n",
      "A94-1016\n",
      "P84-1100\n",
      "W02-1020\n",
      "J85-1003\n",
      "W07-0729\n",
      "J97-2004\n",
      "P04-3001\n",
      "W02-0706\n",
      "P06-2124\n",
      "W98-1005\n",
      "C80-1066\n",
      "D07-1056\n",
      "W07-0723\n",
      "P05-1059\n",
      "W07-0705\n",
      "P04-1066\n",
      "W07-0403\n",
      "C00-2122\n",
      "W03-0302\n",
      "W05-0806\n",
      "W00-0801\n",
      "C90-1016\n",
      "C80-1065\n",
      "C82-1062\n",
      "W95-0115\n",
      "A83-1028\n",
      "H05-1010\n",
      "C86-1073\n",
      "P03-1039\n",
      "J00-1004\n",
      "C00-1015\n",
      "J93-1004\n",
      "W06-3125\n",
      "W05-0826\n",
      "P05-1074\n",
      "C92-2115\n",
      "P97-1062\n",
      "W91-0110\n",
      "N07-1064\n",
      "W01-1405\n",
      "D07-1005\n",
      "C02-1056\n",
      "C04-1047\n",
      "W05-0809\n",
      "C86-1150\n",
      "C90-3101\n",
      "P05-1034\n",
      "P06-1123\n",
      "P01-1008\n",
      "C02-1076\n",
      "W06-3104\n",
      "C96-1078\n",
      "E89-1037\n",
      "P91-1025\n",
      "W99-0602\n",
      "D07-1079\n",
      "J85-2005\n",
      "W02-1019\n",
      "W97-0119\n",
      "H05-1093\n",
      "C92-2113\n",
      "W05-0819\n",
      "C04-1045\n",
      "E91-1048\n",
      "P91-1023\n",
      "E91-1051\n",
      "H01-1033\n",
      "C02-1003\n",
      "C04-1051\n",
      "C04-1073\n",
      "H05-1086\n",
      "W06-3120\n",
      "P05-1057\n",
      "P02-1039\n",
      "N07-1008\n",
      "P84-1099\n",
      "E87-1022\n",
      "H93-1039\n",
      "W01-1411\n",
      "H94-1005\n",
      "P06-1098\n",
      "P06-1091\n",
      "W03-0304\n",
      "W07-0738\n",
      "W06-3124\n",
      "W07-0732\n",
      "N03-1024\n",
      "P08-2010\n",
      "E06-1031\n",
      "N04-1035\n",
      "N03-1017\n",
      "W01-1413\n",
      "W04-3205\n",
      "C04-1006\n",
      "P02-1051\n",
      "P00-1004\n",
      "P00-1056\n",
      "P06-1097\n",
      "E99-1010\n",
      "W05-0824\n",
      "H01-1035\n",
      "W97-0412\n",
      "P82-1001\n",
      "C04-1155\n",
      "P91-1020\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "FW_ABS_Index = {}\n",
    "\n",
    "indicators = ['futur', 'work', 'plan', 'improv', 'explor', 'approach', 'perform', 'research', 'evalu', 'extend', 'would']\n",
    "output_pickle = 'FutureWorkAndAbstractPickle_expanded.pk'\n",
    "#output_pickle = 'temp'\n",
    "input_directory = \"../cleanXMLdataV2/*.out\"\n",
    "#input_directory = \"../cleanXMLdataV2/P11-2088.out\"\n",
    "#read_files = glob.glob(\"/Users/aditi_khullar/Documents/Dropbox/cleanXMLdataV2/*.out\")\n",
    "read_files = glob.glob(input_directory)\n",
    "paperID2label, paperID2title = parse_aan_metadata()\n",
    "for xmlFile in read_files:\n",
    "    paperId = xmlFile.split(\"/\")[-1][0:-4]\n",
    "    if paperId not in common_files:\n",
    "        #only looking at the papers with the labeled information\n",
    "        continue\n",
    "\n",
    "    # DEBUG: looking at the summarization papers \n",
    "    if paperID2label[paperId] != 'M':\n",
    "        continue\n",
    "        \n",
    "    #print paperId\n",
    "    if FW_ABS_Index.has_key(paperId) is False:\n",
    "        FW_ABS_Index[paperId] = [\"Smaple Abstract\", \"Sample Futurework\", \"Sample Intro\", \"Sample Con\"]\n",
    "    with open(xmlFile, 'r') as f:\n",
    "        xmlData = f.read();\n",
    "    #`soup = BeautifulStoneSoup(xmlData, selfClosingTags=['sectionHeader','bodyText'])\n",
    "    soup = BeautifulSoup(xmlData, 'xml')\n",
    "    #print soup.prettify()\n",
    "\n",
    "    #trying to find the title\n",
    "#     title  = soup.find('title')\n",
    "#     if title == None:\n",
    "#         print (\"NOTICE: skipping file:\", xmlFile, \" no TITLE!\")\n",
    "#         continue\n",
    "    #paperTitle = title.get_text()\n",
    "    #print (paperTitle)\n",
    "    \n",
    "    # gets all of the lines after conclusion and before acknowledgement\n",
    "    limit = 5 #assuming that there are not many seperations between conclusion and acknowledgement\n",
    "    bodiesCON = []\n",
    "    bodiesAWK = []\n",
    "    bodyConText = \"\"\n",
    "    for i, header in  enumerate(soup.findAll('sectionHeader')):\n",
    "        #print i\n",
    "        if header['genericHeader'] == 'abstract':    \n",
    "            if header.find_next('bodyText') is not None:\n",
    "                paperAbs = get_abstract(header.find_next('bodyText').get_text())\n",
    "            else:\n",
    "                paperAbs = \" \"\n",
    "            FW_ABS_Index[paperId][0] = paperAbs\n",
    "\n",
    "        if header['genericHeader'] == 'introduction':\n",
    "            FW_ABS_Index[paperId][2] = get_abstract(get_bodyText(header))\n",
    "            \n",
    "        #gets the conclusion section\n",
    "        if header['genericHeader'] == 'conclusions' and float( header['confidence']) > 0.95:\n",
    "            if header.find_next('bodyText') is not None:\n",
    "                paperCon = get_abstract(header.find_next('bodyText').get_text())\n",
    "            else:\n",
    "                paperCon = \" \"\n",
    "\n",
    "            FW_ABS_Index[paperId][3] = paperCon\n",
    "            \n",
    "        #gets all of what we believe is the conclusion section to extract future works in\n",
    "        if header['genericHeader'] == 'conclusions':\n",
    "            if float( header['confidence']) < 0.95:\n",
    "                #bodiesCON = header.find_all_previous('bodyText', limit=3)\n",
    "                #print header['confidence']\n",
    "                bodyConText = get_abstract(prev_bodyText(header))\n",
    "            else:\n",
    "                bodyConText = get_abstract(get_bodyText(header))#bodiesCON +  header.find_all_next('bodyText')\n",
    "    \n",
    "    sentences = print_sentences(bodyConText)\n",
    "    if sentences > 0:\n",
    "        print (paperId)\n",
    "        #write the data to csv format \n",
    "        with open ( 'MachineTranslation_annotation_file.txt', 'a+') as f:\n",
    "            for sent in sentences:\n",
    "                f.write( paperId + \"\\t\" + paperID2title[paperId] + \"\\t\"  + sent.encode('ascii', 'ignore').strip() + \"\\n\")\n",
    "            \n",
    "    futureWorkText = get_futureWork_extended(bodyConText,indicators)\n",
    "    FW_ABS_Index[paperId][1] = futureWorkText\n",
    "\n",
    "# Writing the Dictinary to a pickle file\n",
    "# output = open(output_pickle, 'wb')\n",
    "# pickle.dump(FW_ABS_Index, output)\n",
    "# output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paperID2label = parse_aan_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'the multilingual lexical sample task in senseval-3 featured english ambiguous words that were to be tagged with their most appropriate hindi translation.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FW_ABS_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
